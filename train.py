# -*- coding: utf-8 -*-
"""Self_development (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10xdDpEEWpmrOUlHAlELskyx6aw_cOPxG
"""
'''
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
'''
"""# Libraries and Modules"""

#from __future__ import division
import os
import glob
import numpy as np
import cv2
import torch.utils.data as torch_data
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
import random
import tqdm
from shapely.geometry import Polygon
import os, sys, time, datetime, argparse
from torch.utils.data import DataLoader
from torch.autograd import Variable
import torch.optim as optim
#!pip install terminaltables
from terminaltables import AsciiTable
#!pip install mayavi
#import mayavi.mlab as mlab

"""# Complex Yolo - Constants"""

#root_dir = '/content/drive/MyDrive/KITTI/data'
root_dir = '/data2'
class_list = ["Car", "Pedestrian", "Cyclist"]

CLASS_NAME_TO_ID = {
	'Car': 				0,
	'Pedestrian': 		1,
	'Cyclist': 			2,
	'Van': 				0,
	'Person_sitting': 	1,
}

# Front side (of vehicle) Point Cloud boundary for BEV
boundary = {
    "minX": 0,
    "maxX": 50,
    "minY": -25,
    "maxY": 25,
    "minZ": -2.73,
    "maxZ": 1.27
}

# Back back (of vehicle) Point Cloud boundary for BEV
boundary_back = {
    "minX": -50,
    "maxX": 0,
    "minY": -25,
    "maxY": 25,
    "minZ": -2.73,
    "maxZ": 1.27
}

BEV_WIDTH = 608 # across y axis -25m ~ 25m
BEV_HEIGHT = 608 # across x axis 0m ~ 50m

DISCRETIZATION = (boundary["maxX"] - boundary["minX"])/BEV_HEIGHT

colors = [[0, 255, 255], [0, 0, 255], [255, 0, 0]]

# Following parameters are calculated as an average from KITTI dataset for simplicity
#####################################################################################
Tr_velo_to_cam = np.array([
		[7.49916597e-03, -9.99971248e-01, -8.65110297e-04, -6.71807577e-03],
		[1.18652889e-02, 9.54520517e-04, -9.99910318e-01, -7.33152811e-02],
		[9.99882833e-01, 7.49141178e-03, 1.18719929e-02, -2.78557062e-01],
		[0, 0, 0, 1]
	])

# cal mean from train set
R0 = np.array([
		[0.99992475, 0.00975976, -0.00734152, 0],
		[-0.0097913, 0.99994262, -0.00430371, 0],
		[0.00729911, 0.0043753, 0.99996319, 0],
		[0, 0, 0, 1]
])

P2 = np.array([[719.787081,         0., 608.463003,    44.9538775],
               [        0., 719.787081, 174.545111,     0.1066855],
               [        0.,         0.,         1., 3.0106472e-03],
			   [0., 0., 0., 0]
])

R0_inv = np.linalg.inv(R0)
Tr_velo_to_cam_inv = np.linalg.inv(Tr_velo_to_cam)
P2_inv = np.linalg.pinv(P2)
#####################################################################################

"""# Dataset Preparation - Utility Classes and Functions"""

# Commented out IPython magic to ensure Python compatibility.
def inverse_rigid_trans(Tr):
	''' Inverse a rigid body transform matrix (3x4 as [R|t])
		[R'|-R't; 0|1]
	'''
	inv_Tr = np.zeros_like(Tr) # 3x4
	inv_Tr[0:3,0:3] = np.transpose(Tr[0:3,0:3])
	inv_Tr[0:3,3] = np.dot(-np.transpose(Tr[0:3,0:3]), Tr[0:3,3])
	return inv_Tr

class Object3d(object):
    ''' 3d object label '''
    def __init__(self, label_file_line):
        data = label_file_line.split(' ')
        data[1:] = [float(x) for x in data[1:]]
        # extract label, truncation, occlusion
        self.type = data[0] # 'Car', 'Pedestrian', ...
        self.cls_id = self.cls_type_to_id(self.type)
        self.truncation = data[1] # truncated pixel ratio [0..1]
        self.occlusion = int(data[2]) # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown
        self.alpha = data[3] # object observation angle [-pi..pi]

        # extract 2d bounding box in 0-based coordinates
        self.xmin = data[4] # left
        self.ymin = data[5] # top
        self.xmax = data[6] # right
        self.ymax = data[7] # bottom
        self.box2d = np.array([self.xmin,self.ymin,self.xmax,self.ymax])

        # extract 3d bounding box information
        self.h = data[8] # box height
        self.w = data[9] # box width
        self.l = data[10] # box length (in meters)
        self.t = (data[11],data[12],data[13]) # location (x,y,z) in camera coord.
        self.dis_to_cam = np.linalg.norm(self.t)
        self.ry = data[14] # yaw angle (around Y-axis in camera coordinates) [-pi..pi]
        self.score = data[15] if data.__len__() == 16 else -1.0
        self.level_str = None
        self.level = self.get_obj_level()

    def cls_type_to_id(self, cls_type):
        # Car and Van ==> Car class
        # Pedestrian and Person_Sitting ==> Pedestrian Class
        CLASS_NAME_TO_ID = {
            'Car': 				0,
            'Pedestrian': 		1,
            'Cyclist': 			2,
            'Van': 				0,
            'Person_sitting': 	1
        }
        if cls_type not in CLASS_NAME_TO_ID.keys():
            return -1
        return CLASS_NAME_TO_ID[cls_type]

    def get_obj_level(self):
        height = float(self.box2d[3]) - float(self.box2d[1]) + 1

        if height >= 40 and self.truncation <= 0.15 and self.occlusion <= 0:
            self.level_str = 'Easy'
            return 1  # Easy
        elif height >= 25 and self.truncation <= 0.3 and self.occlusion <= 1:
            self.level_str = 'Moderate'
            return 2  # Moderate
        elif height >= 25 and self.truncation <= 0.5 and self.occlusion <= 2:
            self.level_str = 'Hard'
            return 3  # Hard
        else:
            self.level_str = 'UnKnown'
            return 4

    def print_object(self):
        print('Type, truncation, occlusion, alpha: %s, %d, %d, %f' % \
            (self.type, self.truncation, self.occlusion, self.alpha))
        print('2d bbox (x0,y0,x1,y1): %f, %f, %f, %f' % \
            (self.xmin, self.ymin, self.xmax, self.ymax))
        print('3d bbox h,w,l: %f, %f, %f' % \
            (self.h, self.w, self.l))
        print('3d bbox location, ry: (%f, %f, %f), %f' % \
            (self.t[0],self.t[1],self.t[2],self.ry))

    def to_kitti_format(self):
        kitti_str = '%s %.2f %d %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f %.2f' % (self.type, self.truncation, int(self.occlusion), self.alpha, self.box2d[0], self.box2d[1],
        self.box2d[2], self.box2d[3], self.h, self.w, self.l, self.t[0], self.t[1], self.t[2],self.ry, self.score)
        return kitti_str

class Calibration(Object3d):
    def __init__(self, calib_filepath):
        calibs = self.read_calib_file(calib_filepath)
        # Projection matrix from rect camera coord to image2 coord
        self.P = calibs['P2']
        self.P = np.reshape(self.P, [3,4])
        # Rigid transform from Velodyne coord to reference camera coord
        self.V2C = calibs['Tr_velo2cam']
        self.V2C = np.reshape(self.V2C, [3,4])
        self.C2V = inverse_rigid_trans(self.V2C)
        # Rotation from reference camera coord to rect camera coord
        self.R0 = calibs['R_rect']
        self.R0 = np.reshape(self.R0,[3,3])

        # Camera intrinsics and extrinsics
        self.c_u = self.P[0,2]
        self.c_v = self.P[1,2]
        self.f_u = self.P[0,0]
        self.f_v = self.P[1,1]
        self.b_x = self.P[0,3]/(-self.f_u) # relative
        self.b_y = self.P[1,3]/(-self.f_v)

    def read_calib_file(self, filepath):
        with open(filepath) as f:
            lines = f.readlines()

        obj = lines[2].strip().split(' ')[1:]
        P2 = np.array(obj, dtype=np.float32)
        obj = lines[3].strip().split(' ')[1:]
        P3 = np.array(obj, dtype=np.float32)
        obj = lines[4].strip().split(' ')[1:]
        R0 = np.array(obj, dtype=np.float32)
        obj = lines[5].strip().split(' ')[1:]
        Tr_velo_to_cam = np.array(obj, dtype=np.float32)
        # dictionary of all attributes
        return {'P2': P2.reshape(3, 4),
                'P3': P3.reshape(3, 4),
                'R_rect': R0.reshape(3, 3),
                'Tr_velo2cam': Tr_velo_to_cam.reshape(3, 4)}

    def cart2hom(self, pts_3d):
        """
        :param pts: (N, 3 or 2)
        :return pts_hom: (N, 4 or 3)
        """
        pts_hom = np.hstack((pts_3d, np.ones((pts_3d.shape[0], 1), dtype=np.float32)))
        return pts_hom

    # ===========================
    # ------- 3d to 3d ----------
    # ===========================
    def project_velo_to_ref(self, pts_3d_velo):
        pts_3d_velo = self.cart2hom(pts_3d_velo) # nx4
        return np.dot(pts_3d_velo, np.transpose(self.V2C))

    def project_ref_to_velo(self, pts_3d_ref):
        pts_3d_ref = self.cart2hom(pts_3d_ref) # nx4
        return np.dot(pts_3d_ref, np.transpose(self.C2V))

    def project_rect_to_ref(self, pts_3d_rect):
        ''' Input and Output are nx3 points '''
        return np.transpose(np.dot(np.linalg.inv(self.R0), np.transpose(pts_3d_rect)))

    def project_ref_to_rect(self, pts_3d_ref):
        ''' Input and Output are nx3 points '''
        return np.transpose(np.dot(self.R0, np.transpose(pts_3d_ref)))

    def project_rect_to_velo(self, pts_3d_rect):
        ''' Input: nx3 points in rect camera coord.
            Output: nx3 points in velodyne coord.
        '''
        pts_3d_ref = self.project_rect_to_ref(pts_3d_rect)
        return self.project_ref_to_velo(pts_3d_ref)

    def project_velo_to_rect(self, pts_3d_velo):
        pts_3d_ref = self.project_velo_to_ref(pts_3d_velo)
        return self.project_ref_to_rect(pts_3d_ref)

    # ===========================
    # ------- 3d to 2d ----------
    # ===========================
    def project_rect_to_image(self, pts_3d_rect):
        ''' Input: nx3 points in rect camera coord.
            Output: nx2 points in image2 coord.
        '''
        pts_3d_rect = self.cart2hom(pts_3d_rect)
        pts_2d = np.dot(pts_3d_rect, np.transpose(self.P)) # nx3
        pts_2d[:,0] /= pts_2d[:,2]
        pts_2d[:,1] /= pts_2d[:,2]
        return pts_2d[:,0:2]

    def project_velo_to_image(self, pts_3d_velo):
        ''' Input: nx3 points in velodyne coord.
            Output: nx2 points in image2 coord.
        '''
        pts_3d_rect = self.project_velo_to_rect(pts_3d_velo)
        return self.project_rect_to_image(pts_3d_rect)

    # ===========================
    # ------- 2d to 3d ----------
    # ===========================
    def project_image_to_rect(self, uv_depth):
        ''' Input: nx3 first two channels are uv, 3rd channel
                   is depth in rect camera coord.
            Output: nx3 points in rect camera coord.
        '''
        n = uv_depth.shape[0]
        x = ((uv_depth[:,0]-self.c_u)*uv_depth[:,2])/self.f_u + self.b_x
        y = ((uv_depth[:,1]-self.c_v)*uv_depth[:,2])/self.f_v + self.b_y
        pts_3d_rect = np.zeros((n,3))
        pts_3d_rect[:,0] = x
        pts_3d_rect[:,1] = y
        pts_3d_rect[:,2] = uv_depth[:,2]
        return pts_3d_rect

    def project_image_to_velo(self, uv_depth):
        pts_3d_rect = self.project_image_to_rect(uv_depth)
        return self.project_rect_to_velo(pts_3d_rect)

    def corners3d_to_img_boxes(self, corners3d):
        """
        :param corners3d: (N, 8, 3) corners in rect coordinate
        :return: boxes: (None, 4) [x1, y1, x2, y2] in rgb coordinate
        :return: boxes_corner: (None, 8) [xi, yi] in rgb coordinate
        """
        sample_num = corners3d.shape[0]
        corners3d_hom = np.concatenate((corners3d, np.ones((sample_num, 8, 1))), axis=2)  # (N, 8, 4)

        img_pts = np.matmul(corners3d_hom, self.P.T)  # (N, 8, 3)

        x, y = img_pts[:, :, 0] / img_pts[:, :, 2], img_pts[:, :, 1] / img_pts[:, :, 2]
        x1, y1 = np.min(x, axis=1), np.min(y, axis=1)
        x2, y2 = np.max(x, axis=1), np.max(y, axis=1)

        boxes = np.concatenate((x1.reshape(-1, 1), y1.reshape(-1, 1), x2.reshape(-1, 1), y2.reshape(-1, 1)), axis=1)
        boxes_corner = np.concatenate((x.reshape(-1, 8, 1), y.reshape(-1, 8, 1)), axis=2)
        return boxes, boxes_corner

def read_label(label_filename):
    lines = [line.rstrip() for line in open(label_filename)]
    objects = [Object3d(line) for line in lines]
    return objects

def angle_in_limit(angle):
  # To limit the angle in -pi/2 - pi/2
  limit_degree = 5
  while angle >= np.pi / 2:
    angle -= np.pi
  while angle < -np.pi / 2:
    angle += np.pi
  if abs(angle + np.pi / 2) < limit_degree / 180 * np.pi:
    angle = np.pi / 2
  return angle

def removePoints(PointCloud, BoundaryCond):
    # Boundary condition
    minX = BoundaryCond['minX']
    maxX = BoundaryCond['maxX']
    minY = BoundaryCond['minY']
    maxY = BoundaryCond['maxY']
    minZ = BoundaryCond['minZ']
    maxZ = BoundaryCond['maxZ']

    # Remove the point out of range x,y,z
    mask = np.where((PointCloud[:, 0] >= minX) & (PointCloud[:, 0] <= maxX) & (PointCloud[:, 1] >= minY) & (
            PointCloud[:, 1] <= maxY) & (PointCloud[:, 2] >= minZ) & (PointCloud[:, 2] <= maxZ))
    PointCloud = PointCloud[mask]

    PointCloud[:, 2] = PointCloud[:, 2] - minZ

    return PointCloud

def makeBVFeature(PointCloud_, Discretization, bc):
    '''The first step in the process to create a BEV feature map is discretization. It is done to map the points’
    position to pixel position without losing any fine details. BEV map is represented as a grid of cells, where each cell represents
    a specific area in the surroundings.
    Each point in the point cloud should be assigned to the nearest discrete cell. Hence, np.floor is used to round the results to the nearest integer.'''
    Height =  BEV_HEIGHT + 1
    Width =  BEV_WIDTH + 1

    # Discretize Feature Map
    PointCloud = np.copy(PointCloud_)
    PointCloud[:, 0] = np.int_(np.floor(PointCloud[:, 0] / Discretization))
    PointCloud[:, 1] = np.int_(np.floor(PointCloud[:, 1] / Discretization) + Width / 2)

    # sort-3times
    indices = np.lexsort((-PointCloud[:, 2], PointCloud[:, 1], PointCloud[:, 0]))
    PointCloud = PointCloud[indices]

    # Height Map
    heightMap = np.zeros((Height, Width))

    _, indices = np.unique(PointCloud[:, 0:2], axis=0, return_index=True)
    PointCloud_frac = PointCloud[indices]
    # some important problem is image coordinate is (y,x), not (x,y)
    max_height = float(np.abs(bc['maxZ'] - bc['minZ']))
    heightMap[np.int_(PointCloud_frac[:, 0]), np.int_(PointCloud_frac[:, 1])] = PointCloud_frac[:, 2] / max_height

    # Intensity Map & DensityMap
    intensityMap = np.zeros((Height, Width))
    densityMap = np.zeros((Height, Width))

    _, indices, counts = np.unique(PointCloud[:, 0:2], axis=0, return_index=True, return_counts=True)
    PointCloud_top = PointCloud[indices]

    normalizedCounts = np.minimum(1.0, np.log(counts + 1) / np.log(64))

    intensityMap[np.int_(PointCloud_top[:, 0]), np.int_(PointCloud_top[:, 1])] = PointCloud_top[:, 3]
    densityMap[np.int_(PointCloud_top[:, 0]), np.int_(PointCloud_top[:, 1])] = normalizedCounts

    RGB_Map = np.zeros((3, Height - 1, Width - 1))
    RGB_Map[2, :, :] = densityMap[: BEV_HEIGHT, : BEV_WIDTH]  # r_map
    RGB_Map[1, :, :] = heightMap[: BEV_HEIGHT, : BEV_WIDTH]  # g_map
    RGB_Map[0, :, :] = intensityMap[: BEV_HEIGHT, : BEV_WIDTH]  # b_map

    return RGB_Map

def build_yolo_target(labels):
    bc =  boundary
    target = np.zeros([50, 7], dtype=np.float32)

    index = 0
    for i in range(labels.shape[0]):
        cl, x, y, z, h, w, l, yaw = labels[i]

        # ped and cyc labels are very small, so lets add some factor to height/width
        l = l + 0.3
        w = w + 0.3

        yaw = np.pi * 2 - yaw
        if (x > bc["minX"]) and (x < bc["maxX"]) and (y > bc["minY"]) and (y < bc["maxY"]):
            y1 = (y - bc["minY"]) / (bc["maxY"]-bc["minY"])  # we should put this in [0,1], so divide max_size  80 m
            x1 = (x - bc["minX"]) / (bc["maxX"]-bc["minX"])  # we should put this in [0,1], so divide max_size  40 m
            w1 = w / (bc["maxY"] - bc["minY"])
            l1 = l / (bc["maxX"] - bc["minX"])

            target[index][0] = cl
            target[index][1] = y1
            target[index][2] = x1
            target[index][3] = w1
            target[index][4] = l1
            target[index][5] = math.sin(float(yaw))
            target[index][6] = math.cos(float(yaw))

            index = index+1

    return target

def resize(image, size):
    image = F.interpolate(image.unsqueeze(0), size=size, mode="nearest").squeeze(0)
    return image

def read_labels_for_bevbox(objects):
    bbox_selected = []
    for obj in objects:
        if obj.cls_id != -1:
            bbox = []
            bbox.append(obj.cls_id)
            bbox.extend([obj.t[0], obj.t[1], obj.t[2], obj.h, obj.w, obj.l, obj.ry])
            bbox_selected.append(bbox)

    if (len(bbox_selected) == 0):
        return np.zeros((1, 8), dtype=np.float32), True
    else:
        bbox_selected = np.array(bbox_selected).astype(np.float32)
        return bbox_selected, False

def camera_to_lidar(x, y, z, V2C=None,R0=None,P2=None):
	p = np.array([x, y, z, 1])
	if V2C is None or R0 is None:
		p = np.matmul(R0_inv, p)
		p = np.matmul(Tr_velo_to_cam_inv, p)
	else:
		R0_i = np.zeros((4,4))
		R0_i[:3,:3] = R0
		R0_i[3,3] = 1
		p = np.matmul(np.linalg.inv(R0_i), p)
		p = np.matmul(inverse_rigid_trans(V2C), p)
	p = p[0:3]
	return tuple(p)


def camera_to_lidar_box(boxes, V2C=None, R0=None, P2=None):
	# (N, 7) -> (N, 7) x,y,z,h,w,l,r
	ret = []
	for box in boxes:
		x, y, z, h, w, l, ry = box
		(x, y, z), h, w, l, rz = camera_to_lidar(
			x, y, z,V2C=V2C, R0=R0, P2=P2), h, w, l, -ry - np.pi / 2
		#rz = angle_in_limit(rz)
		ret.append([x, y, z, h, w, l, rz])
	return np.array(ret).reshape(-1, 7)

"""# Dataset Preparation for Complex-Yolo"""

"""# for training set 2:"""

class Dataset(torch_data.Dataset):
  def __init__(self, dir_root, split, folder=None, multiscale=False, mode= 'TRAIN'):
      # part 1
    self.split = split
    self.multiscale = multiscale
    self.img_size =  BEV_WIDTH
    self.max_objects = 100
    self.min_size = self.img_size - 3 * 32
    self.max_size = self.img_size + 3 * 32
    self.batch_count = 0
    self.imageset_dir = 'data2/ImageSets'
    print(self.imageset_dir)
    # self.lidar_path = os.path.join(self.imageset_dir, "velodyne")
    # self.image_path = os.path.join(self.imageset_dir, "image_2")
    # self.calib_path = os.path.join(self.imageset_dir, "calib")
    # self.label_path = os.path.join(self.imageset_dir, "label_2")
    self.lidar_path = 'data2/object/training/velodyne'
    self.image_path = 'data2/object/training/image_2'
    self.calib_path = 'data2/object/training/calib'
    self.label_path = 'data2/object/training/label_2'
    if mode == 'TEST':
      self.lidar_path = 'data2/object/testing/velodyne'
      self.image_path = 'data2/object/testing/image_2'
      self.calib_path = 'data2/object/testning/calib'

    if self.split == "train":
      #split_directory = os.path.join(root_dir, 'KITTI', 'ImageSets', split+ '.txt')
      split_directory = '/home/ubuntu/safa/3D-Object-Detection-Using-KITTI-Dataset/data2/ImageSets/train.txt'
      self.image_id_list = [x.strip() for x in open(split_directory).readlines()]
    else:
      bin_files = glob.glob("%s/*.bin" % self.lidar_path) # using wild card pattern for matching
      self.files = sorted(bin_files)
      self.image_id_list = [os.path.split(x)[1].split(".")[0].strip() for x in self.files]
      print(self.image_id_list[0])
    self.total_samples = self.image_id_list.__len__()
    #  part 2
    assert mode in ['TRAIN', 'EVAL', 'TEST'], 'Invalid mode: %s' % mode
    self.mode = mode
    self.sample_id_list = []
    if mode == 'TRAIN':
        self.preprocess_yolo_training_data()
    else:
        self.sample_id_list = [int(sample_id) for sample_id in self.image_id_list]
    print('Load %s samples from %s' % (mode, self.imageset_dir))
    print('Done: total %s samples %d' % (mode, len(self.sample_id_list)))

    def get_image(self, id):
      # Construct the path to the image file using the given id
      img_file = os.path.join(self.image_path, '%06d.png' % id)
      # Check if the image file exists; raise an AssertionError if it doesn't
      assert os.path.exists(img_file)
      # Read and return the image using OpenCV
      return cv2.imread(img_file)

  def get_image_shape(self, id):
      # Get the image using the get_image method
      img = self.get_image(id)
      # Extract and return the width, height, and number of channels of the image
      width, height, channel = img.shape
      return width, height, channel

  def get_lidar(self, id):
      # Construct the path to the lidar file using the given id
      lidar_file = os.path.join(self.lidar_path, '%06d.bin' % id)
      # Check if the lidar file exists; raise an AssertionError if it doesn't
      assert os.path.exists(lidar_file)
      # Read the binary data from the lidar file and reshape it into a 2D numpy array
      # The data is interpreted as a 1D array of float32 values and then reshaped into a 2D array with 4 columns
      return np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)

  def get_calib(self, idx):
        calib_file = os.path.join(self.calib_path, '%06d.txt' % idx)
        assert os.path.exists(calib_file)
        return  Calibration(calib_file) # calling calibration class

  def get_label(self, idx):
        label_file = os.path.join(self.label_path, '%06d.txt' % idx)
        assert os.path.exists(label_file)
        return  read_label(label_file)

  def preprocess_yolo_training_data(self):
        """
        Discard samples which don't have current training class objects, which will not be used for training.
        Valid sample_id is stored in self.sample_id_list
        """
        for id in range(0, self.total_samples):
            sample_no = int(self.image_id_list[id])
            objects = self.get_label(sample_no)
            calib = self.get_calib(sample_no)
            # reading labels for BEV boxes
            bbox_selected = []
            for obj in objects:
              if obj.cls_id != -1:
                bbox = []
                bbox.append(obj.cls_id)
                bbox.extend([obj.t[0], obj.t[1], obj.t[2], obj.h, obj.w, obj.l, obj.ry])
                bbox_selected.append(bbox)

            if (len(bbox_selected) == 0):
              labels = np.zeros((1, 8), dtype=np.float32)
              noObjects = True
            else:
              bbox_selected = np.array(bbox_selected).astype(np.float32)
              labels = bbox_selected
              noObjects = False
            # Camera to Lidar box conversion
            if not noObjects:
              # convert rect cam to velo cord
              boxes = labels[:, 1:]
              V2C = calib.V2C
              R0 = calib.R0
              P2 = calib.P
              ret = []
              for box in boxes:
                  x, y, z, h, w, l, ry = box
                  p = np.array([x, y, z, 1])
                  if V2C is None or R0 is None:
                    p = np.matmul(R0_inv, p)
                    p = np.matmul(Tr_velo_to_cam_inv, p)
                  else:
                    R0_i = np.zeros((4,4))
                    R0_i[:3,:3] = R0
                    R0_i[3,3] = 1
                    p = np.matmul(np.linalg.inv(R0_i), p)
                    ''' Inverse a rigid body transform matrix (3x4 as [R|t])
                      [R'|-R't; 0|1]
                    '''
                    inv_Tr = np.zeros_like(V2C) # 3x4
                    inv_Tr[0:3,0:3] = np.transpose(V2C[0:3,0:3])
                    inv_Tr[0:3,3] = np.dot(-np.transpose(V2C[0:3,0:3]), V2C[0:3,3])
                    p = np.matmul(inv_Tr, p)
                    p = p[0:3]

                    (x, y, z), h, w, l, rz = tuple(p), h, w, l, -ry - np.pi / 2
                  rz = angle_in_limit(rz)
                  ret.append([x, y, z, h, w, l, rz])
              labels[:, 1:] = np.array(ret).reshape(-1, 7)
            # Checking and Holding only those samples that exist is classes we are on working
            valid_list = []
            for i in range(labels.shape[0]):
              if int(labels[i, 0]) in  CLASS_NAME_TO_ID.values():
                if self.check_pc_range(labels[i, 1:4]) is True:
                    valid_list.append(labels[i,0])
            if len(valid_list):
              self.sample_id_list.append(sample_no)

  def check_pc_range(self,xyz):
        x_range = [ boundary["minX"],  boundary["maxX"]]
        y_range = [ boundary["minY"],  boundary["maxY"]]
        z_range = [ boundary["minZ"],  boundary["maxZ"]]
        if (x_range[0] <= xyz[0] <= x_range[1]) and (y_range[0] <= xyz[1] <= y_range[1]) and (z_range[0] <= xyz[2] <= z_range[1]):
            return True
        return False
  def __getitem__(self, index):
        sample_id = int(self.sample_id_list[index])
        if self.mode in ['TRAIN', 'EVAL']:
          lidarData = self.get_lidar(sample_id)
          objects = self.get_label(sample_id)
          calib = self.get_calib(sample_id)
          # reading labels for BEV boxes
          labels, noObjectLabels =  read_labels_for_bevbox(objects)
          if not noObjectLabels:
            labels[:, 1:] =  camera_to_lidar_box(labels[:, 1:], calib.V2C, calib.R0, calib.P)  # convert rect cam to velo cord

          # bbox_selected = []
          # for obj in objects:
          #   if obj.cls_id != -1:
          #     bbox = []
          #     bbox.append(obj.cls_id)
          #     bbox.extend([obj.t[0], obj.t[1], obj.t[2], obj.h, obj.w, obj.l, obj.ry])
          #     bbox_selected.append(bbox)
          # if (len(bbox_selected) == 0):
          #   labels = np.zeros((1, 8), dtype=np.float32)
          #   noObjects = True
          # else:
          #   bbox_selected = np.array(bbox_selected).astype(np.float32)
          #   labels = bbox_selected
          #   noObjects = False
          # # Camera to Lidar box conversion
          # if not noObjects:
          #   # convert rect cam to velo cord
          #   boxes = labels[:, 1:]
          #   V2C = calib.V2C
          #   R0 = calib.R0
          #   P2 = calib.P
          #   ret = []
          #   for box in boxes:
          #     x, y, z, h, w, l, ry = box
          #     p = np.array([x, y, z, 1])
          #     if V2C is None or R0 is None:
          #       p = np.matmul(R0_inv, p)
          #       p = np.matmul(Tr_velo_to_cam_inv, p)
          #     else:
          #       R0_i = np.zeros((4,4))
          #       R0_i[:3,:3] = R0
          #       R0_i[3,3] = 1
          #       p = np.matmul(np.linalg.inv(R0_i), p)
          #       ''' Inverse a rigid body transform matrix (3x4 as [R|t])
          #         [R'|-R't; 0|1]
          #       '''
          #       inv_Tr = np.zeros_like(V2C) # 3x4
          #       inv_Tr[0:3,0:3] = np.transpose(V2C[0:3,0:3])
          #       inv_Tr[0:3,3] = np.dot(-np.transpose(V2C[0:3,0:3]), V2C[0:3,3])
          #       p = np.matmul(inv_Tr, p)
          #       p = p[0:3]
          #       (x, y, z), h, w, l, rz = tuple(p), h, w, l, -ry - np.pi / 2
          #       rz = angle_in_limit(rz)
          #       ret.append([x, y, z, h, w, l, rz])
          #     print(np.array(ret).shape)
          #     labels[:, 1:] = np.array(ret).reshape(-1, 7)
              ##########################################
            # It is a useful practice to limit the point cloud data to concentrate on a specific region.
            # Hence, a filter is created which is used to keep the points only in the required range as mentioned in the paper.
            b =  removePoints(lidarData,  boundary)
            #MakeBVFeature
            #To map the point cloud points to pixel position, a function called makeBVFeature is created.
            #It takes in three arguments: point cloud data, discretization factor, and boundary range.
            #Since the x and y ranges are the same, the discretization factor is calculated as:
            #DISCRETIZATION = (boundary[‘maxX’] — boundary[‘minX] / BEV_HEIGHT
            rgb_map =  makeBVFeature(b,  DISCRETIZATION,  boundary)

            target =  build_yolo_target(labels)
            img_file = os.path.join(self.image_path, '%06d.png' % sample_id)
            ntargets = 0
            for i, t in enumerate(target):
                if t.sum(0):
                  ntargets += 1
            targets = torch.zeros((ntargets, 8))
            for i, t in enumerate(target):
              if t.sum(0):
                targets[i, 1:] = torch.from_numpy(t)
            img = torch.from_numpy(rgb_map).type(torch.FloatTensor)
            return img_file, img, targets
        else:
          lidarData = self.get_lidar(sample_id)
          b =  removePoints(lidarData,  boundary)
          rgb_map =  makeBVFeature(b,  DISCRETIZATION,  boundary)
          img_file = os.path.join(self.image_path, '%06d.png' % sample_id)
          return img_file, rgb_map

  def collate_fn(self, batch):
        paths, imgs, targets = list(zip(*batch))
        # Remove empty placeholder targets
        targets = [boxes for boxes in targets if boxes is not None]
        # Add sample index to targets
        for i, boxes in enumerate(targets):
            boxes[:, 0] = i
        targets = torch.cat(targets, 0)
        # Selects new image size every tenth batch
        if self.multiscale and self.batch_count % 10 == 0:
            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))
        # Resize images to input shape
        imgs = torch.stack([resize(img, self.img_size) for img in imgs])
        self.batch_count += 1
        return paths, imgs, targets
  def __len__(self):
        return len(self.sample_id_list)

"""# Model Development - Complex Yolo

## Utility functions
"""

def convert_format(boxes_array):
    """
    :param array: an array of shape [# bboxs, 4, 2]
    :return: a shapely.geometry.Polygon object
    """
    polygons = [Polygon([(box[i, 0], box[i, 1]) for i in range(4)]) for box in boxes_array]
    return np.array(polygons)

def compute_iou(box, boxes):
    """Calculates IoU of the given box with the array of the given boxes.
    box: a polygon
    boxes: a vector of polygons
    Note: the areas are passed in rather than calculated here for
    efficiency. Calculate once in the caller to avoid duplicate work.
    """
    # Calculate intersection areas
    iou = [box.intersection(b).area / (box.union(b).area + 1e-12) for b in boxes]

    return np.array(iou, dtype=np.float32)


# bev image coordinates format
def get_corners(x, y, w, l, yaw):
    bev_corners = np.zeros((4, 2), dtype=np.float32)

    # front left
    bev_corners[0, 0] = x - w / 2 * np.cos(yaw) - l / 2 * np.sin(yaw)
    bev_corners[0, 1] = y - w / 2 * np.sin(yaw) + l / 2 * np.cos(yaw)

    # rear left
    bev_corners[1, 0] = x - w / 2 * np.cos(yaw) + l / 2 * np.sin(yaw)
    bev_corners[1, 1] = y - w / 2 * np.sin(yaw) - l / 2 * np.cos(yaw)

    # rear right
    bev_corners[2, 0] = x + w / 2 * np.cos(yaw) + l / 2 * np.sin(yaw)
    bev_corners[2, 1] = y + w / 2 * np.sin(yaw) - l / 2 * np.cos(yaw)

    # front right
    bev_corners[3, 0] = x + w / 2 * np.cos(yaw) - l / 2 * np.sin(yaw)
    bev_corners[3, 1] = y + w / 2 * np.sin(yaw) + l / 2 * np.cos(yaw)

    return bev_corners


def rotated_bbox_iou_polygon(box1, box2):
    box1 = to_cpu(box1).numpy()
    box2 = to_cpu(box2).numpy()

    x,y,w,l,im,re = box1
    angle = np.arctan2(im, re)
    bbox1 = np.array( get_corners(x, y, w, l, angle)).reshape(-1,4,2)
    bbox1 = convert_format(bbox1)

    bbox2 = []
    for i in range(box2.shape[0]):
        x,y,w,l,im,re = box2[i,:]
        angle = np.arctan2(im, re)
        bev_corners = get_corners(x, y, w, l, angle)
        bbox2.append(bev_corners)
    bbox2 = convert_format(np.array(bbox2))

    return compute_iou(bbox1[0], bbox2)


def rotated_box_wh_iou_polygon(anchor, wh, imre):
    w1, h1, im1, re1 = anchor[0], anchor[1], anchor[2], anchor[3]

    wh = wh.t()
    imre = imre.t()
    w2, h2, im2, re2 = wh[0], wh[1], imre[0], imre[1]

    anchor_box = torch.cuda.FloatTensor([100, 100, w1, h1, im1, re1]).view(-1, 6)
    target_boxes = torch.cuda.FloatTensor(w2.shape[0], 6).fill_(100)

    target_boxes[:, 2] = w2
    target_boxes[:, 3] = h2
    target_boxes[:, 4] = im2
    target_boxes[:, 5] = re2

    ious = rotated_bbox_iou_polygon(anchor_box[0], target_boxes)

    return torch.from_numpy(ious)



def rotated_box_11_iou_polygon(box1, box2, nG):

    box1_new = torch.cuda.FloatTensor(box1.shape[0], 6).fill_(0)
    box2_new = torch.cuda.FloatTensor(box2.shape[0], 6).fill_(0)

    box1_new[:, :4] = box1[:, :4]
    box1_new[:, 4:] = box1[:, 4:]

    box2_new[:, :4] = box2[:, :4] * nG
    box2_new[:, 4:] = box2[:, 4:]

    ious = []
    for i in range(box1_new.shape[0]):
        bbox1 = box1_new[i]
        bbox2 = box2_new[i].view(-1, 6)

        iou = rotated_bbox_iou_polygon(bbox1, bbox2).squeeze()
        ious.append(iou)

    ious = np.array(ious)

    return torch.from_numpy(ious)

def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):

    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor
    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.ByteTensor

    nB = pred_boxes.size(0)
    nA = pred_boxes.size(1)
    nC = pred_cls.size(-1)
    nG = pred_boxes.size(2)

    # Output tensors
    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)
    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)
    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)
    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)
    tx = FloatTensor(nB, nA, nG, nG).fill_(0)
    ty = FloatTensor(nB, nA, nG, nG).fill_(0)
    tw = FloatTensor(nB, nA, nG, nG).fill_(0)
    th = FloatTensor(nB, nA, nG, nG).fill_(0)
    tim = FloatTensor(nB, nA, nG, nG).fill_(0)
    tre = FloatTensor(nB, nA, nG, nG).fill_(0)
    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)

    # Convert to position relative to box
    target_boxes = target[:, 2:8]

    gxy = target_boxes[:, :2] * nG
    gwh = target_boxes[:, 2:4] * nG
    gimre = target_boxes[:, 4:]

    # Get anchors with best iou
    ious = torch.stack([rotated_box_wh_iou_polygon(anchor, gwh, gimre) for anchor in anchors])

    best_ious, best_n = ious.max(0)
    b, target_labels = target[:, :2].long().t()

    gx, gy = gxy.t()
    gw, gh = gwh.t()
    gim, gre = gimre.t()
    gi, gj = gxy.long().t()
    # Set masks
    obj_mask[b, best_n, gj, gi] = 1
    noobj_mask[b, best_n, gj, gi] = 0

    # Set noobj mask to zero where iou exceeds ignore threshold
    for i, anchor_ious in enumerate(ious.t()):
        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0

    # Coordinates
    tx[b, best_n, gj, gi] = gx - gx.floor()
    ty[b, best_n, gj, gi] = gy - gy.floor()
    # Width and height
    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)
    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)
    # Im and real part
    tim[b, best_n, gj, gi] = gim
    tre[b, best_n, gj, gi] = gre

    # One-hot encoding of label
    tcls[b, best_n, gj, gi, target_labels] = 1
    # Compute label correctness and iou at best anchor
    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()

    rotated_iou_scores = rotated_box_11_iou_polygon(pred_boxes[b, best_n, gj, gi], target_boxes, nG)
    iou_scores[b, best_n, gj, gi] = rotated_iou_scores.to('cuda:0')

    tconf = obj_mask.float()
    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tim, tre, tcls, tconf


def to_cpu(tensor):
    return tensor.detach().cpu()

def parse_model_config(path):
    """Parses the yolo-v3 layer configuration file and returns module definitions"""
    file = open(path, 'r')
    lines = file.read().split('\n')
    lines = [x for x in lines if x and not x.startswith('#')]
    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces
    module_defs = []
    for line in lines:
        if line.startswith('['): # This marks the start of a new block
            module_defs.append({})
            module_defs[-1]['type'] = line[1:-1].rstrip()
            if module_defs[-1]['type'] == 'convolutional':
                module_defs[-1]['batch_normalize'] = 0
        else:
            key, value = line.split("=")
            value = value.strip()
            module_defs[-1][key.rstrip()] = value.strip()

    return module_defs

def create_modules(module_defs):
    """
    Constructs module list of layer blocks from module configuration in module_defs
    """
    hyperparams = module_defs.pop(0)
    output_filters = [int(hyperparams["channels"])]
    module_list = nn.ModuleList()
    for module_i, module_def in enumerate(module_defs):
        modules = nn.Sequential()

        if module_def["type"] == "convolutional":
            bn = int(module_def["batch_normalize"])
            filters = int(module_def["filters"])
            kernel_size = int(module_def["size"])
            pad = (kernel_size - 1) // 2
            modules.add_module(
                f"conv_{module_i}",
                nn.Conv2d(
                    in_channels=output_filters[-1],
                    out_channels=filters,
                    kernel_size=kernel_size,
                    stride=int(module_def["stride"]),
                    padding=pad,
                    bias=not bn,
                ),
            )
            if bn:
                modules.add_module(f"batch_norm_{module_i}", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))
            if module_def["activation"] == "leaky":
                modules.add_module(f"leaky_{module_i}", nn.LeakyReLU(0.1))

        elif module_def["type"] == "maxpool":
            kernel_size = int(module_def["size"])
            stride = int(module_def["stride"])
            if kernel_size == 2 and stride == 1:
                modules.add_module(f"_debug_padding_{module_i}", nn.ZeroPad2d((0, 1, 0, 1)))
            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))
            modules.add_module(f"maxpool_{module_i}", maxpool)

        elif module_def["type"] == "upsample":
            upsample = Upsample(scale_factor=int(module_def["stride"]), mode="nearest")
            modules.add_module(f"upsample_{module_i}", upsample)

        elif module_def["type"] == "route":
            layers = [int(x) for x in module_def["layers"].split(",")]
            filters = sum([output_filters[1:][i] for i in layers])
            modules.add_module(f"route_{module_i}", EmptyLayer())

        elif module_def["type"] == "shortcut":
            filters = output_filters[1:][int(module_def["from"])]
            modules.add_module(f"shortcut_{module_i}", EmptyLayer())

        elif module_def["type"] == "yolo":
            anchor_idxs = [int(x) for x in module_def["mask"].split(",")]
            # Extract anchors
            anchors = [float(x) for x in module_def["anchors"].split(",")]
            anchors = [(anchors[i], anchors[i + 1], math.sin(anchors[i + 2]), math.cos(anchors[i + 2])) for i in range(0, len(anchors), 3)]
            anchors = [anchors[i] for i in anchor_idxs]
            num_classes = int(module_def["classes"])
            img_size = int(hyperparams["height"])
            # Define detection layer
            yolo_layer = YOLOLayer(anchors, num_classes, img_size)
            modules.add_module(f"yolo_{module_i}", yolo_layer)
        # Register module list and number of output filters
        module_list.append(modules)
        output_filters.append(filters)

    return hyperparams, module_list


class Upsample(nn.Module):
    """ nn.Upsample is deprecated """

    def __init__(self, scale_factor, mode="nearest"):
        super(Upsample, self).__init__()
        self.scale_factor = scale_factor
        self.mode = mode

    def forward(self, x):
        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)
        return x


class EmptyLayer(nn.Module):
    """Placeholder for 'route' and 'shortcut' layers"""

    def __init__(self):
        super(EmptyLayer, self).__init__()


class YOLOLayer(nn.Module):
    """Detection layer"""

    def __init__(self, anchors, num_classes, img_dim=416):
        super(YOLOLayer, self).__init__()
        self.anchors = anchors
        self.num_anchors = len(anchors)
        self.num_classes = num_classes
        self.ignore_thres = 0.5
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCELoss()
        self.obj_scale = 1
        self.noobj_scale = 100
        self.metrics = {}
        self.img_dim = img_dim
        self.grid_size = 0  # grid size

    def compute_grid_offsets(self, grid_size, cuda=True):
        self.grid_size = grid_size
        g = self.grid_size
        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor
        self.stride = self.img_dim / self.grid_size
        # Calculate offsets for each grid
        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)
        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)
        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride, im, re) for a_w, a_h, im, re in self.anchors])
        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))
        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))

    def forward(self, x, targets=None, img_dim=None):

        # Tensors for cuda support
        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor
        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor
        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor

        self.img_dim = img_dim
        num_samples = x.size(0)
        grid_size = x.size(2)

        prediction = (
            x.view(num_samples, self.num_anchors, self.num_classes + 7, grid_size, grid_size)
            .permute(0, 1, 3, 4, 2)
            .contiguous()
        )

        # Get outputs
        x = torch.sigmoid(prediction[..., 0])  # Center x
        y = torch.sigmoid(prediction[..., 1])  # Center y
        w = prediction[..., 2]  # Width
        h = prediction[..., 3]  # Height
        im = prediction[..., 4]  # angle imaginary part
        re = prediction[..., 5]  # angle real part
        pred_conf = torch.sigmoid(prediction[..., 6])  # Conf
        pred_cls = torch.sigmoid(prediction[..., 7:])  # Cls pred.

        # If grid size does not match current we compute new offsets
        if grid_size != self.grid_size:
            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)

        # Add offset and scale with anchors
        pred_boxes = FloatTensor(prediction[..., :6].shape)
        pred_boxes[..., 0] = x.data + self.grid_x
        pred_boxes[..., 1] = y.data + self.grid_y
        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w
        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h
        pred_boxes[..., 4] = im
        pred_boxes[..., 5] = re

        output = torch.cat(
            (
                #pred_boxes.view(num_samples, -1, 6) * self.stride,
                pred_boxes[..., :4].view(num_samples, -1, 4) * self.stride,
                pred_boxes[..., 4:].view(num_samples, -1, 2),
                pred_conf.view(num_samples, -1, 1),
                pred_cls.view(num_samples, -1, self.num_classes),
            ),
            -1,
        )

        if targets is None:
            return output, 0
        else:
            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tim, tre, tcls, tconf = build_targets(
                pred_boxes=pred_boxes,
                pred_cls=pred_cls,
                target=targets,
                anchors=self.scaled_anchors,
                ignore_thres=self.ignore_thres,
            )

            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)
            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])
            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])
            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])
            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])
            loss_im = self.mse_loss(im[obj_mask], tim[obj_mask])
            loss_re = self.mse_loss(re[obj_mask], tre[obj_mask])
            loss_eular = loss_im + loss_re
            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])
            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])
            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj
            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])
            total_loss = loss_x + loss_y + loss_w + loss_h + loss_eular + loss_conf + loss_cls

            # Metrics
            cls_acc = 100 * class_mask[obj_mask].mean()
            conf_obj = pred_conf[obj_mask].mean()
            conf_noobj = pred_conf[noobj_mask].mean()
            conf50 = (pred_conf > 0.5).float()
            iou50 = (iou_scores > 0.5).float()
            iou75 = (iou_scores > 0.75).float()
            detected_mask = conf50 * class_mask * tconf
            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)
            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)
            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)

            self.metrics = {
                "loss": to_cpu(total_loss).item(),
                "x": to_cpu(loss_x).item(),
                "y": to_cpu(loss_y).item(),
                "w": to_cpu(loss_w).item(),
                "h": to_cpu(loss_h).item(),
                "im": to_cpu(loss_im).item(),
                "re": to_cpu(loss_re).item(),
                "conf": to_cpu(loss_conf).item(),
                "cls": to_cpu(loss_cls).item(),
                "cls_acc": to_cpu(cls_acc).item(),
                "recall50": to_cpu(recall50).item(),
                "recall75": to_cpu(recall75).item(),
                "precision": to_cpu(precision).item(),
                "conf_obj": to_cpu(conf_obj).item(),
                "conf_noobj": to_cpu(conf_noobj).item(),
                "grid_size": grid_size,
            }

            return output, total_loss


class Darknet(nn.Module):
    """YOLOv2 object detection model"""

    def __init__(self, config_path, img_size=416):
        super(Darknet, self).__init__()
        self.module_defs = parse_model_config(config_path) # calling parse function from utility cell
        self.hyperparams, self.module_list = create_modules(self.module_defs) # calling create module function
        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], "metrics")]
        self.img_size = img_size
        self.seen = 0
        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)

    def forward(self, x, targets=None):
        img_dim = x.shape[2]
        loss = 0
        layer_outputs, yolo_outputs = [], []
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if module_def["type"] in ["convolutional", "upsample", "maxpool"]:
                x = module(x)
            elif module_def["type"] == "route":
                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def["layers"].split(",")], 1)
            elif module_def["type"] == "shortcut":
                layer_i = int(module_def["from"])
                x = layer_outputs[-1] + layer_outputs[layer_i]
            elif module_def["type"] == "yolo":
                x, layer_loss = module[0](x, targets, img_dim)
                loss += layer_loss
                yolo_outputs.append(x)
            layer_outputs.append(x)
        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))
        return yolo_outputs if targets is None else (loss, yolo_outputs)

    def load_darknet_weights(self, weights_path):
        """Parses and loads the weights stored in 'weights_path'"""

        # Open the weights file
        with open(weights_path, "rb") as f:
            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values
            self.header_info = header  # Needed to write header when saving weights
            self.seen = header[3]  # number of images seen during training
            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights

        # Establish cutoff for loading backbone weights
        cutoff = None
        if "darknet53.conv.74" in weights_path:
            cutoff = 75
        elif "yolov3-tiny.conv.15" in weights_path:
            cutoff = 15

        ptr = 0
        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):
            if i == cutoff:
                break
            if module_def["type"] == "convolutional":
                conv_layer = module[0]
                if module_def["batch_normalize"]:
                    # Load BN bias, weights, running mean and running variance
                    bn_layer = module[1]
                    num_b = bn_layer.bias.numel()  # Number of biases
                    # Bias
                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)
                    bn_layer.bias.data.copy_(bn_b)
                    ptr += num_b
                    # Weight
                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)
                    bn_layer.weight.data.copy_(bn_w)
                    ptr += num_b
                    # Running Mean
                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)
                    bn_layer.running_mean.data.copy_(bn_rm)
                    ptr += num_b
                    # Running Var
                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)
                    bn_layer.running_var.data.copy_(bn_rv)
                    ptr += num_b
                else:
                    # Load conv. bias
                    num_b = conv_layer.bias.numel()
                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)
                    conv_layer.bias.data.copy_(conv_b)
                    ptr += num_b
                # Load conv. weights
                num_w = conv_layer.weight.numel()
                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)
                conv_layer.weight.data.copy_(conv_w)
                ptr += num_w

"""# Evaluation_Metrices"""

def compute_ap(recall, precision):
    """ Compute the average precision, given the recall and precision curves.
    Code originally from https://github.com/rbgirshick/py-faster-rcnn.

    # Arguments
        recall:    The recall curve (list).
        precision: The precision curve (list).
    # Returns
        The average precision as computed in py-faster-rcnn.

    This function calculates the Average Precision (AP) given the recall and precision curves.
    It sorts the recall and precision curves together based on decreasing recall values.
    It calculates the area under the Precision-Recall curve (PR curve) to get the AP.
    """
    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.0], recall, [1.0]))
    mpre = np.concatenate(([0.0], precision, [0.0]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap

def get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold):
    """ Compute true positives, predicted scores and predicted labels per sample
    This function computes various evaluation metrics for a single batch of predictions.
    It iterates through each sample in the batch and checks if there are any predictions.
    For each prediction, it calculates Intersection-over-Union (IoU) with ground truth
    boxes using a custom rotated_bbox_iou function.
    It identifies true positives based on the IoU threshold.
    Finally, it returns a list containing true positives, predicted scores, and predicted labels for each sample. """
    batch_metrics = []
    for sample_i in range(len(outputs)):

        if outputs[sample_i] is None:
            continue

        output = outputs[sample_i]
        pred_boxes = output[:, :6]
        pred_scores = output[:, 6]
        pred_labels = output[:, -1]

        true_positives = np.zeros(pred_boxes.shape[0])

        annotations = targets[targets[:, 0] == sample_i][:, 1:]
        target_labels = annotations[:, 0] if len(annotations) else []
        if len(annotations):
            detected_boxes = []
            target_boxes = annotations[:, 1:]

            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):

                # If targets are found break
                if len(detected_boxes) == len(annotations):
                    break

                # Ignore if label is not one of the target labels
                if pred_label not in target_labels:
                    continue

                #iou, box_index = rotated_bbox_iou(pred_box.unsqueeze(0), target_boxes, 1.0, False).squeeze().max(0)
                ious = rotated_bbox_iou_polygon(pred_box, target_boxes)
                iou, box_index = torch.from_numpy(ious).max(0)

                if iou >= iou_threshold and box_index not in detected_boxes:
                    true_positives[pred_i] = 1
                    detected_boxes += [box_index]
        batch_metrics.append([true_positives, pred_scores, pred_labels])
    return batch_metrics

def non_max_suppression_rotated_bbox(prediction, conf_thres=0.95, nms_thres=0.4):
    """
        Removes detections with lower object confidence score than 'conf_thres' and performs
        Non-Maximum Suppression to further filter detections.
        Returns detections with shape:
            (x, y, w, l, im, re, object_conf, class_score, class_pred)
      This function performs Non-Maximum Suppression (NMS) on a batch of detections.
      It removes detections with confidence scores below a threshold (conf_thres).
      It sorts the remaining detections by confidence score in descending order.
      It iterates through detections and keeps the one with the highest confidence.
      For overlapping detections with similar class labels (using another custom rotated_bbox_iou function),
      it removes lower confidence detections while updating the remaining one's bounding box based on a weighted average.
    """

    output = [None for _ in range(len(prediction))]
    for image_i, image_pred in enumerate(prediction):
        # Filter out confidence scores below threshold
        image_pred = image_pred[image_pred[:, 6] >= conf_thres]
        # If none are remaining => process next image
        if not image_pred.size(0):
            continue
        # Object confidence times class confidence
        score = image_pred[:, 6] * image_pred[:, 7:].max(1)[0]
        # Sort by it
        image_pred = image_pred[(-score).argsort()]
        class_confs, class_preds = image_pred[:, 7:].max(1, keepdim=True)
        detections = torch.cat((image_pred[:, :7].float(), class_confs.float(), class_preds.float()), 1)
        # Perform non-maximum suppression
        keep_boxes = []
        while detections.size(0):
            #large_overlap = rotated_bbox_iou(detections[0, :6].unsqueeze(0), detections[:, :6], 1.0, False) > nms_thres # not working
            large_overlap = rotated_bbox_iou_polygon(detections[0, :6], detections[:, :6]) > nms_thres
            # large_overlap = torch.from_numpy(large_overlap.astype('uint8'))
            large_overlap = torch.from_numpy(large_overlap)
            label_match = detections[0, -1] == detections[:, -1]
            # Indices of boxes with lower confidence scores, large IOUs and matching labels
            invalid = large_overlap & label_match
            weights = detections[invalid, 6:7]
            # Merge overlapping bboxes by order of confidence
            detections[0, :6] = (weights * detections[invalid, :6]).sum(0) / weights.sum()
            keep_boxes += [detections[0]]
            detections = detections[~invalid]
        if keep_boxes:
            output[image_i] = torch.stack(keep_boxes)

    return output

def ap_per_class(tp, conf, pred_cls, target_cls):
    """ Compute the average precision, given the recall and precision curves.
    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.
    # Arguments
        tp:    True positives (list).
        conf:  Objectness value from 0-1 (list).
        pred_cls: Predicted object classes (list).
        target_cls: True object classes (list).
    # Returns
        The average precision as computed in py-faster-rcnn.

    This function calculates the Average Precision (AP), precision, recall, and
     F1 score for each class in a dataset, given true positives (TP), confidence
     scores (conf), predicted classes (pred_cls), and target classes (target_cls).
    """

    # Sort by objectness
    i = np.argsort(-conf)
    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]

    # Find unique classes
    unique_classes = np.unique(target_cls)

    # Create Precision-Recall curve and compute AP for each class
    ap, p, r = [], [], []
    for c in tqdm.tqdm(unique_classes, desc="Computing AP"):
        i = pred_cls == c
        n_gt = (target_cls == c).sum()  # Number of ground truth objects
        n_p = i.sum()  # Number of predicted objects

        if n_p == 0 and n_gt == 0:
            continue
        elif n_p == 0 or n_gt == 0:
            ap.append(0)
            r.append(0)
            p.append(0)
        else:
            # Accumulate FPs and TPs
            fpc = (1 - tp[i]).cumsum()
            tpc = (tp[i]).cumsum()

            # Recall
            recall_curve = tpc / (n_gt + 1e-16)
            r.append(recall_curve[-1])

            # Precision
            precision_curve = tpc / (tpc + fpc)
            p.append(precision_curve[-1])

            # AP from recall-precision curve
            ap.append(compute_ap(recall_curve, precision_curve))

    # Compute F1 score (harmonic mean of precision and recall)
    p, r, ap = np.array(p), np.array(r), np.array(ap)
    f1 = 2 * p * r / (p + r + 1e-16)

    return p, r, ap, f1, unique_classes.astype("int32")


def evaluate(model, iou_thres, conf_thres, nms_thres, img_size, batch_size):
    '''This function evaluates the object detection model's performance on a validation dataset.
     It calculates metrics like precision, recall, Average Precision (AP), and F1 score for each class.'''
    model.eval()
    # Get dataloader
    split='valid'
    dataset = Dataset( root_dir, split=split, mode='EVAL', folder='training')
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=False, num_workers=1, collate_fn=dataset.collate_fn
    )

    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor

    labels = []
    sample_metrics = []  # List of tuples (TP, confs, pred)
    for batch_i, (_, imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc="Detecting objects")):

        # Extract labels
        labels += targets[:, 1].tolist()
        # Rescale target
        targets[:, 2:] *= img_size

        imgs = Variable(imgs.type(Tensor), requires_grad=False)

        with torch.no_grad():
            outputs = model(imgs)
            outputs = non_max_suppression_rotated_bbox(outputs, conf_thres=conf_thres, nms_thres=nms_thres)

        sample_metrics += get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold=iou_thres)

    # Concatenate sample statistics
    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]
    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)

    return precision, recall, AP, f1, ap_class


"""# For new training set:"""

if __name__ == "__main__":

    epochs = 100
    batch_size = 4
    gradient_accumulations=2
    model_def='yolo_configuration/complex_yolov3.cfg'
    n_cpu = 4
    img_size=BEV_WIDTH
    evaluation_interval=2
    multiscale_training=True
    #model_path = "/content/drive/MyDrive/Kitti/model"
    #pretrained_weights
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    class_names = ['Car','Pedestrian','Cyclist']

    # Initiate model
    model = Darknet(model_def, img_size=img_size).to(device)
    #model.load_state_dict(torch.load('model_stateV3', map_location=torch.device('cpu')))

    model.load_state_dict(torch.load('model_stateV3'))

    # Get dataloader
    dataset = Dataset(
        root_dir,
        split='train',
        mode='TRAIN',
        folder='training',
        multiscale=multiscale_training
    )

    dataloader = DataLoader(
        dataset,
         batch_size,
        shuffle=True,
        num_workers=n_cpu,
        pin_memory=True,
        collate_fn=dataset.collate_fn
    )

    optimizer = torch.optim.Adam(model.parameters())

    metrics = [
        "grid_size",
        "loss",
        "x",
        "y",
        "w",
        "h",
        "im",
        "re",
        "conf",
        "cls",
        "cls_acc",
        "recall50",
        "recall75",
        "precision",
        "conf_obj",
        "conf_noobj",
    ]
    count = 0
    max_mAP = 0.0
    for epoch in range(0, epochs, 1):
        print(count)
        count +=1
        model.train()
        for batch_i, (_, imgs, targets) in enumerate(dataloader):
            batches_done = len(dataloader) * epoch + batch_i

            imgs = Variable(imgs.to(device))
            targets = Variable(targets.to(device), requires_grad=False)

            loss, outputs = model(imgs, targets)
            loss.backward()

            if batches_done %  gradient_accumulations:
                # Accumulates gradient before each step
                optimizer.step()
                optimizer.zero_grad()
            model.seen += imgs.size(0)

        if epoch %  evaluation_interval == 0:
            print("\n---- Evaluating Model ----")
            # Evaluate the model on the validation set
            precision, recall, AP, f1, ap_class = evaluate(
                model,
                iou_thres=0.5,
                conf_thres=0.5,
                nms_thres=0.5,
                img_size= img_size,
                batch_size=8,
            )
            evaluation_metrics = [
                ("val_precision", precision.mean()),
                ("val_recall", recall.mean()),
                ("val_mAP", AP.mean()),
                ("val_f1", f1.mean()),
            ]

            # Print class APs and mAP
            ap_table = [["Index", "Class name", "AP"]]
            for i, c in enumerate(ap_class):
                ap_table += [[c, class_names[c], "%.5f" % AP[i]]]
            print(AsciiTable(ap_table).table)
            print(f"---- mAP {AP.mean()}")

            #if epoch %  checkpoint_interval == 0:
            if AP.mean() > max_mAP:
                #torch.save(model.state_dict(), f"checkpoints/yolov3_ckpt_epoch-%d_MAP-%.2f.pth" % (epoch, AP.mean()))
                max_mAP = AP.mean()

torch.save(model.state_dict(),'model_statefinal')
torch.save(model, 'entire_modelfinal')

"""# Testing

## Utilities
"""
# '''
# def inverse_yolo_target(targets, bc):
#     ntargets = 0
#     for i, t in enumerate(targets):
#         if t.sum(0):ntargets += 1

#     labels = np.zeros([ntargets, 8], dtype=np.float32)

#     n = 0
#     for t in targets:
#         if t.sum(0) == 0:
#             continue

#         c, y, x, w, l, im, re = t
#         z, h = -1.55, 1.5
#         if c == 1:
#             h = 1.8
#         elif c == 2:
#             h = 1.4

#         y = y * (bc["maxY"] - bc["minY"]) + bc["minY"]
#         x = x * (bc["maxX"] - bc["minX"]) + bc["minX"]
#         w = w * (bc["maxY"] - bc["minY"])
#         l = l * (bc["maxX"] - bc["minX"])

#         w -= 0.3
#         l -= 0.3

#         labels[n, :] = c, x, y, z, h, w, l, - np.arctan2(im, re) - 2*np.pi
#         n += 1

#     return labels
# def lidar_to_camera(x, y, z,V2C=None, R0=None, P2=None):
# 	p = np.array([x, y, z, 1])
# 	if V2C is None or R0 is None:
# 		p = np.matmul(Tr_velo_to_cam, p)
# 		p = np.matmul(R0, p)
# 	else:
# 		p = np.matmul(V2C, p)
# 		p = np.matmul(R0, p)
# 	p = p[0:3]
# 	return tuple(p)

# def lidar_to_camera_box(boxes,V2C=None, R0=None, P2=None):
# 	# (N, 7) -> (N, 7) x,y,z,h,w,l,r
# 	ret = []
# 	for box in boxes:
# 		x, y, z, h, w, l, rz = box
# 		(x, y, z), h, w, l, ry = lidar_to_camera(
# 			x, y, z,V2C=V2C, R0=R0, P2=P2), h, w, l, -rz - np.pi / 2
# 		#ry = angle_in_limit(ry)
# 		ret.append([x, y, z, h, w, l, ry])
# 	return np.array(ret).reshape(-1, 7)

# def project_to_image(pts_3d, P):
#     ''' Project 3d points to image plane.

#     Usage: pts_2d = projectToImage(pts_3d, P)
#       input: pts_3d: nx3 matrix
#              P:      3x4 projection matrix
#       output: pts_2d: nx2 matrix

#       P(3x4) dot pts_3d_extended(4xn) = projected_pts_2d(3xn)
#       => normalize projected_pts_2d(2xn)

#       <=> pts_3d_extended(nx4) dot P'(4x3) = projected_pts_2d(nx3)
#           => normalize projected_pts_2d(nx2)
#     '''
#     n = pts_3d.shape[0]
#     pts_3d_extend = np.hstack((pts_3d, np.ones((n,1))))
#     #print(('pts_3d_extend shape: ', pts_3d_extend.shape))
#     pts_2d = np.dot(pts_3d_extend, np.transpose(P)) # nx3
#     pts_2d[:,0] /= pts_2d[:,2]
#     pts_2d[:,1] /= pts_2d[:,2]
#     return pts_2d[:,0:2]

# def roty(t):
#     #Rotation about the y-axis.
#     c = np.cos(t)
#     s = np.sin(t)
#     return np.array([[c,  0,  s],
#                      [0,  1,  0],
#                      [-s, 0,  c]])

# def compute_box_3d(obj, P):
#     ''' Takes an object and a projection matrix (P) and projects the 3d
#         bounding box into the image plane.
#         Returns:
#             corners_2d: (8,2) array in left image coord.
#             corners_3d: (8,3) array in in rect camera coord.
#     '''
#     # compute rotational matrix around yaw axis
#     R = roty(obj.ry)

#     # 3d bounding box dimensions
#     l = obj.l
#     w = obj.w
#     h = obj.h

#     # 3d bounding box corners
#     x_corners = [l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]
#     y_corners = [0,0,0,0,-h,-h,-h,-h]
#     z_corners = [w/2,-w/2,-w/2,w/2,w/2,-w/2,-w/2,w/2]

#     # rotate and translate 3d bounding box
#     corners_3d = np.dot(R, np.vstack([x_corners,y_corners,z_corners]))
#     #print corners_3d.shape
#     corners_3d[0,:] = corners_3d[0,:] + obj.t[0]
#     corners_3d[1,:] = corners_3d[1,:] + obj.t[1]
#     corners_3d[2,:] = corners_3d[2,:] + obj.t[2]
#     #print 'cornsers_3d: ', corners_3d
#     # only draw 3d bounding box for objs in front of the camera
#     if np.any(corners_3d[2,:]<0.1):
#         corners_2d = None
#         return corners_2d, np.transpose(corners_3d)

#     # project the 3d bounding box into the image plane
#     corners_2d = project_to_image(np.transpose(corners_3d), P)
#     #print 'corners_2d: ', corners_2d
#     return corners_2d, np.transpose(corners_3d)

# def read_labels_for_bevbox(objects):
#     bbox_selected = []
#     for obj in objects:
#         if obj.cls_id != -1:
#             bbox = []
#             bbox.append(obj.cls_id)
#             bbox.extend([obj.t[0], obj.t[1], obj.t[2], obj.h, obj.w, obj.l, obj.ry])
#             bbox_selected.append(bbox)

#     if (len(bbox_selected) == 0):
#         return np.zeros((1, 8), dtype=np.float32), True
#     else:
#         bbox_selected = np.array(bbox_selected).astype(np.float32)
#         return bbox_selected, False


# def camera_to_lidar(x, y, z, V2C=None,R0=None,P2=None):
# 	p = np.array([x, y, z, 1])
# 	if V2C is None or R0 is None:
# 		p = np.matmul(R0_inv, p)
# 		p = np.matmul(Tr_velo_to_cam_inv, p)
# 	else:
# 		R0_i = np.zeros((4,4))
# 		R0_i[:3,:3] = R0
# 		R0_i[3,3] = 1
# 		p = np.matmul(np.linalg.inv(R0_i), p)
# 		p = np.matmul(inverse_rigid_trans(V2C), p)
# 	p = p[0:3]
# 	return tuple(p)

# def camera_to_lidar_box(boxes, V2C=None, R0=None, P2=None):
# 	# (N, 7) -> (N, 7) x,y,z,h,w,l,r
# 	ret = []
# 	for box in boxes:
# 		x, y, z, h, w, l, ry = box
# 		(x, y, z), h, w, l, rz = camera_to_lidar(
# 			x, y, z,V2C=V2C, R0=R0, P2=P2), h, w, l, -ry - np.pi / 2
# 		#rz = angle_in_limit(rz)
# 		ret.append([x, y, z, h, w, l, rz])
# 	return np.array(ret).reshape(-1, 7)


# #send parameters in bev image coordinates format
# def drawRotatedBox(img,x,y,w,l,yaw,color):
#     bev_corners = get_corners(x, y, w, l, yaw)
#     corners_int = bev_corners.reshape(-1, 1, 2).astype(int)
#     print(corners_int,corners_int.shape)
#     cv2.polylines(img, [corners_int], True, color, 2)
#     corners_int = bev_corners.reshape(-1, 2)
#     cv2.line(img, (int(corners_int[0, 0]), int(corners_int[0, 1])), (int(corners_int[3, 0]), int(corners_int[3, 1])), (255, 255, 0), 2)
#     #cv2.line(img, (corners_int[0, 0], corners_int[0, 1]), (corners_int[3, 0], corners_int[3, 1]), (255, 255, 0), 2)

# def draw_box_in_bev(rgb_map, target):
#     for j in range(50):
#         if(np.sum(target[j,1:]) == 0):continue
#         cls_id = int(target[j][0])
#         x = target[j][1] *  BEV_WIDTH
#         y = target[j][2] *  BEV_HEIGHT
#         w = target[j][3] *  BEV_WIDTH
#         l = target[j][4] *  BEV_HEIGHT
#         yaw = np.arctan2(target[j][5], target[j][6])
#         drawRotatedBox(rgb_map, x, y, w, l, yaw,  colors[cls_id])

# def rescale_boxes(boxes, current_dim, original_shape):
#     """ Rescales bounding boxes to the original shape """
#     orig_h, orig_w = original_shape
#     # The amount of padding that was added
#     pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))
#     pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))
#     # Image height and width after padding is removed
#     unpad_h = current_dim - pad_y
#     unpad_w = current_dim - pad_x
#     # Rescale bounding boxes to dimension of original image
#     boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w
#     boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h
#     boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w
#     boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h

#     return boxes
# def compute_orientation_3d(obj, P):
#     ''' Takes an object and a projection matrix (P) and projects the 3d
#         object orientation vector into the image plane.
#         Returns:
#             orientation_2d: (2,2) array in left image coord.
#             orientation_3d: (2,3) array in in rect camera coord.
#     '''

#     # compute rotational matrix around yaw axis
#     R = roty(obj.ry)

#     # orientation in object coordinate system
#     orientation_3d = np.array([[0.0, obj.l],[0,0],[0,0]])

#     # rotate and translate in camera coordinate system, project in image
#     orientation_3d = np.dot(R, orientation_3d)
#     orientation_3d[0,:] = orientation_3d[0,:] + obj.t[0]
#     orientation_3d[1,:] = orientation_3d[1,:] + obj.t[1]
#     orientation_3d[2,:] = orientation_3d[2,:] + obj.t[2]

#     # vector behind image plane?
#     if np.any(orientation_3d[2,:]<0.1):
#       orientation_2d = None
#       return orientation_2d, np.transpose(orientation_3d)

#     # project orientation into the image plane
#     orientation_2d = project_to_image(np.transpose(orientation_3d), P)
#     return orientation_2d, np.transpose(orientation_3d)

# def draw_projected_box3d(image, qs, color=(255,0,255), thickness=2):
#     ''' Draw 3d bounding box in image
#         qs: (8,3) array of vertices for the 3d box in following order:
#             1 -------- 0
#            /|         /|
#           2 -------- 3 .
#           | |        | |
#           . 5 -------- 4
#           |/         |/
#           6 -------- 7
#     '''
#     qs = qs.astype(np.int32)
#     for k in range(0,4):
#        # Ref: http://docs.enthought.com/mayavi/mayavi/auto/mlab_helper_functions.html
#        i,j=k,(k+1)%4
#        # use LINE_AA for opencv3
#        cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness)

#        i,j=k+4,(k+1)%4 + 4
#        cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness)

#        i,j=k,k+4
#        cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness)
#     return image

# def draw_lidar_simple(pc, color=None):
#     ''' Draw lidar points. simplest set up. '''
#     fig = mlab.figure(figure=None, bgcolor=(0,0,0), fgcolor=None, engine=None, size=(1600, 1000))
#     if color is None: color = pc[:,2]
#     #draw points
#     mlab.points3d(pc[:,0], pc[:,1], pc[:,2], color, color=None, mode='point', colormap = 'gnuplot', scale_factor=1, figure=fig)
#     #draw origin
#     mlab.points3d(0, 0, 0, color=(1,1,1), mode='sphere', scale_factor=0.2)
#     #draw axis
#     axes=np.array([
#         [2.,0.,0.,0.],
#         [0.,2.,0.,0.],
#         [0.,0.,2.,0.],
#     ],dtype=np.float64)
#     mlab.plot3d([0, axes[0,0]], [0, axes[0,1]], [0, axes[0,2]], color=(1,0,0), tube_radius=None, figure=fig)
#     mlab.plot3d([0, axes[1,0]], [0, axes[1,1]], [0, axes[1,2]], color=(0,1,0), tube_radius=None, figure=fig)
#     mlab.plot3d([0, axes[2,0]], [0, axes[2,1]], [0, axes[2,2]], color=(0,0,1), tube_radius=None, figure=fig)
#     mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=62.0, figure=fig)
#     return fig

# def draw_lidar(pc, color=None, fig1=None, bgcolor=(0,0,0), pts_scale=1, pts_mode='point', pts_color=None):
#     ''' Draw lidar points
#     Args:
#         pc: numpy array (n,3) of XYZ
#         color: numpy array (n) of intensity or whatever
#         fig: mayavi figure handler, if None create new one otherwise will use it
#     Returns:
#         fig: created or used fig
#     '''
#     #if fig1 is None: fig1 = mlab.figure(figure="point cloud", bgcolor=bgcolor, fgcolor=None, engine=None, size=(1600, 1000))

#     mlab.clf(figure=None)
#     if color is None: color = pc[:,2]
#     mlab.points3d(pc[:,0], pc[:,1], pc[:,2], color, color=pts_color, mode=pts_mode, colormap = 'gnuplot', scale_factor=pts_scale, figure=fig1)

#     #draw origin
#     mlab.points3d(0, 0, 0, color=(1,1,1), mode='sphere', scale_factor=0.2)

#     #draw axis
#     axes=np.array([
#         [2.,0.,0.,0.],
#         [0.,2.,0.,0.],
#         [0.,0.,2.,0.],
#     ],dtype=np.float64)

#     mlab.plot3d([0, axes[0,0]], [0, axes[0,1]], [0, axes[0,2]], color=(1,0,0), tube_radius=None, figure=fig1)
#     mlab.plot3d([0, axes[1,0]], [0, axes[1,1]], [0, axes[1,2]], color=(0,1,0), tube_radius=None, figure=fig1)
#     mlab.plot3d([0, axes[2,0]], [0, axes[2,1]], [0, axes[2,2]], color=(0,0,1), tube_radius=None, figure=fig1)

#     # draw fov (todo: update to real sensor spec.)
#     fov=np.array([  # 45 degree
#         [20., 20., 0.,0.],
#         [20.,-20., 0.,0.],
#     ],dtype=np.float64)

#     mlab.plot3d([0, fov[0,0]], [0, fov[0,1]], [0, fov[0,2]], color=(1,1,1), tube_radius=None, line_width=1, figure=fig1)
#     mlab.plot3d([0, fov[1,0]], [0, fov[1,1]], [0, fov[1,2]], color=(1,1,1), tube_radius=None, line_width=1, figure=fig1)

#     # draw square region
#     TOP_Y_MIN=-20
#     TOP_Y_MAX=20
#     TOP_X_MIN=0
#     TOP_X_MAX=40
#     TOP_Z_MIN=-2.0
#     TOP_Z_MAX=0.4

#     x1 = TOP_X_MIN
#     x2 = TOP_X_MAX
#     y1 = TOP_Y_MIN
#     y2 = TOP_Y_MAX
#     mlab.plot3d([x1, x1], [y1, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig1)
#     mlab.plot3d([x2, x2], [y1, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig1)
#     mlab.plot3d([x1, x2], [y1, y1], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig1)
#     mlab.plot3d([x1, x2], [y2, y2], [0,0], color=(0.5,0.5,0.5), tube_radius=0.1, line_width=1, figure=fig1)

#     #mlab.orientation_axes()
#     mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=60.0, figure=fig1)
#     return fig1

# def draw_gt_boxes3d(gt_boxes3d, fig, color=(1,1,1), line_width=2, draw_text=True, text_scale=(1,1,1), color_list=None):
#     ''' Draw 3D bounding boxes
#     Args:
#         gt_boxes3d: numpy array (n,8,3) for XYZs of the box corners
#         fig: mayavi figure handler
#         color: RGB value tuple in range (0,1), box line color
#         line_width: box line width
#         draw_text: boolean, if true, write box indices beside boxes
#         text_scale: three number tuple
#         color_list: a list of RGB tuple, if not None, overwrite color.
#     Returns:
#         fig: updated fig
#     '''
#     num = len(gt_boxes3d)
#     for n in range(num):
#         b = gt_boxes3d[n]
#         if color_list is not None:
#             color = color_list[n]
#         if draw_text: mlab.text3d(b[4,0], b[4,1], b[4,2], '%d'%n, scale=text_scale, color=color, figure=fig)
#         for k in range(0,4):
#             #http://docs.enthought.com/mayavi/mayavi/auto/mlab_helper_functions.html
#             i,j=k,(k+1)%4
#             mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)

#             i,j=k+4,(k+1)%4 + 4
#             mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)

#             i,j=k,k+4
#             mlab.plot3d([b[i,0], b[j,0]], [b[i,1], b[j,1]], [b[i,2], b[j,2]], color=color, tube_radius=None, line_width=line_width, figure=fig)
#     #mlab.show(1)
#     #mlab.view(azimuth=180, elevation=70, focalpoint=[ 12.0909996 , -1.04700089, -2.03249991], distance=62.0, figure=fig)
#     return fig

# def get_lidar_in_image_fov(pc_velo, calib, xmin, ymin, xmax, ymax,
#                            return_more=False, clip_distance=0.0):
#     ''' Filter lidar points, keep those in image FOV '''
#     pts_2d = calib.project_velo_to_image(pc_velo)
#     fov_inds = (pts_2d[:,0]<xmax) & (pts_2d[:,0]>=xmin) & \
#         (pts_2d[:,1]<ymax) & (pts_2d[:,1]>=ymin)
#     fov_inds = fov_inds & (pc_velo[:,0]>clip_distance)
#     imgfov_pc_velo = pc_velo[fov_inds,:]
#     if return_more:
#         return imgfov_pc_velo, pts_2d, fov_inds
#     else:
#         return imgfov_pc_velo

# def show_image_with_boxes(img, objects, calib, show3d=False):
#     ''' Show image with 2D bounding boxes '''

#     img2 = np.copy(img) # for 3d bbox
#     for obj in objects:
#         if obj.type=='DontCare':continue
#         #cv2.rectangle(img2, (int(obj.xmin),int(obj.ymin)),
#         #    (int(obj.xmax),int(obj.ymax)), (0,255,0), 2)
#         box3d_pts_2d, box3d_pts_3d = compute_box_3d(obj, calib.P)
#         if box3d_pts_2d is not None:
#             img2 = draw_projected_box3d(img2, box3d_pts_2d, colors[obj.cls_id])
#     if show3d:
#         cv2.imshow("img", img2)
#     return img2

# def show_lidar_with_boxes(pc_velo, objects, calib,
#                           img_fov=False, img_width=None, img_height=None, fig=None):
#     ''' Show all LiDAR points.
#         Draw 3d box in LiDAR point cloud (in velo coord system) '''

#     if not fig:
#         fig = mlab.figure(figure="KITTI_POINT_CLOUD", bgcolor=(0,0,0), fgcolor=None, engine=None, size=(1250, 550))

#     if img_fov:
#         pc_velo = get_lidar_in_image_fov(pc_velo, calib, 0, 0, img_width, img_height)

#     draw_lidar(pc_velo, fig1=fig)

#     for obj in objects:

#         if obj.type=='DontCare':continue
#         # Draw 3d bounding box
#         box3d_pts_2d, box3d_pts_3d = compute_box_3d(obj, calib.P)
#         box3d_pts_3d_velo = calib.project_rect_to_velo(box3d_pts_3d)

#         # Draw heading arrow
#         ori3d_pts_2d, ori3d_pts_3d = compute_orientation_3d(obj, calib.P)
#         ori3d_pts_3d_velo = calib.project_rect_to_velo(ori3d_pts_3d)
#         x1,y1,z1 = ori3d_pts_3d_velo[0,:]
#         x2,y2,z2 = ori3d_pts_3d_velo[1,:]

#         draw_gt_boxes3d([box3d_pts_3d_velo], fig=fig, color=(0,1,1), line_width=2, draw_text=False)

#         mlab.plot3d([x1, x2], [y1, y2], [z1,z2], color=(0.5,0.5,0.5), tube_radius=None, line_width=1, figure=fig)

#     mlab.view(distance=90)

# from google.colab.patches import cv2_imshow

# # Replace the following line
# # cv2.imshow("bev img", RGB_Map)
# # with
# #cv2_imshow(RGB_Map)

# def predictions_to_kitti_format(img_detections, calib, img_shape_2d, img_size, RGB_Map=None):
#     predictions = np.zeros([50, 7], dtype=np.float32)
#     count = 0
#     for detections in img_detections:
#         if detections is None:
#             continue
#         # Rescale boxes to original image
#         for x, y, w, l, im, re, conf, cls_conf, cls_pred in detections:
#             yaw = np.arctan2(im, re)
#             predictions[count, :] = cls_pred, x/img_size, y/img_size, w/img_size, l/img_size, im, re
#             count += 1
#             print("ITS COUNTTTTTTTTTT: ", count)

#     predictions = inverse_yolo_target(predictions, boundary)
#     if predictions.shape[0]:
#         predictions[:, 1:] = lidar_to_camera_box(predictions[:, 1:], calib.V2C, calib.R0, calib.P)

#     objects_new = []
#     corners3d = []
#     for index, l in enumerate(predictions):

#         str = "Pedestrian"
#         if l[0] == 0:str="Car"
#         elif l[0] == 1:str="Pedestrian"
#         elif l[0] == 2: str="Cyclist"
#         else:str = "DontCare"
#         line = '%s -1 -1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0' % str

#         obj = Object3d(line)
#         obj.t = l[1:4]
#         obj.h,obj.w,obj.l = l[4:7]
#         obj.ry = np.arctan2(math.sin(l[7]), math.cos(l[7]))

#         _, corners_3d = compute_box_3d(obj, calib.P)
#         corners3d.append(corners_3d)
#         objects_new.append(obj)

#     if len(corners3d) > 0:
#         corners3d = np.array(corners3d)
#         img_boxes, _ = calib.corners3d_to_img_boxes(corners3d)

#         img_boxes[:, 0] = np.clip(img_boxes[:, 0], 0, img_shape_2d[1] - 1)
#         img_boxes[:, 1] = np.clip(img_boxes[:, 1], 0, img_shape_2d[0] - 1)
#         img_boxes[:, 2] = np.clip(img_boxes[:, 2], 0, img_shape_2d[1] - 1)
#         img_boxes[:, 3] = np.clip(img_boxes[:, 3], 0, img_shape_2d[0] - 1)

#         img_boxes_w = img_boxes[:, 2] - img_boxes[:, 0]
#         img_boxes_h = img_boxes[:, 3] - img_boxes[:, 1]
#         box_valid_mask = np.logical_and(img_boxes_w < img_shape_2d[1] * 0.8, img_boxes_h < img_shape_2d[0] * 0.8)

#     for i, obj in enumerate(objects_new):
#         x, z, ry = obj.t[0], obj.t[2], obj.ry
#         beta = np.arctan2(z, x)
#         alpha = -np.sign(beta) * np.pi / 2 + beta + ry

#         obj.alpha = alpha
#         obj.box2d = img_boxes[i, :]

#     if RGB_Map is not None:
#         labels, noObjectLabels = read_labels_for_bevbox(objects_new)
#         if not noObjectLabels:
#             labels[:, 1:] = camera_to_lidar_box(labels[:, 1:], calib.V2C, calib.R0, calib.P) # convert rect cam to velo cord

#         target = build_yolo_target(labels)
#         draw_box_in_bev(RGB_Map, target)

#     return objects_new

# if __name__ == "__main__":
#     model_def = "/content/drive/MyDrive/KITTI/yolo_configuration/complex_yolov3.cfg"
#     #weights_path = "checkpoints/tiny-yolov3_ckpt_epoch-220.pth"
#     model_path = '/content/model_stateV2'
#     conf_thres = 0.5
#     nms_thres = 0.5
#     img_size = BEV_WIDTH
#     split = 'valid'
#     folder = 'training'
#     classes = ['Car','Pedestrian','Cyclist']
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     # Set up model
#     model = Darknet( model_def, img_size= img_size).to(device)
#     # Load checkpoint weights
#     model.load_state_dict(torch.load( model_path,map_location=torch.device('cpu')))
#     # Eval mode
#     model.eval()

#     dataset = Dataset(root_dir, split= split, mode='TEST', folder= folder)
#     data_loader = torch_data.DataLoader(dataset, 1, shuffle=False)

#     Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor

#     start_time = time.time()
#     for index, (img_paths, bev_maps) in enumerate(data_loader):

#         # Configure bev image
#         input_imgs = Variable(bev_maps.type(Tensor))

#         # Get detections
#         with torch.no_grad():
#             detections = model(input_imgs)
#             detections = non_max_suppression_rotated_bbox(detections,  conf_thres,  nms_thres)

#         end_time = time.time()
#         print(f"FPS: {(1.0/(end_time-start_time)):0.2f}")
#         start_time = end_time

#         img_detections = []  # Stores detections for each image index
#         img_detections.extend(detections)

#         bev_maps = torch.squeeze(bev_maps).numpy()

#         RGB_Map = np.zeros((BEV_WIDTH, BEV_WIDTH, 3))
#         RGB_Map[:, :, 2] = bev_maps[0, :, :]  # r_map
#         RGB_Map[:, :, 1] = bev_maps[1, :, :]  # g_map
#         RGB_Map[:, :, 0] = bev_maps[2, :, :]  # b_map

#         RGB_Map *= 255
#         RGB_Map = RGB_Map.astype(np.uint8)

#         for detections in img_detections:
#             if detections is None:
#                 continue

#             # Rescale boxes to original image
#             detections = rescale_boxes(detections,  img_size, RGB_Map.shape[:2])
#             for x, y, w, l, im, re, conf, cls_conf, cls_pred in detections:
#                 yaw = np.arctan2(im, re)
#                 # Draw rotated box
#                 drawRotatedBox(RGB_Map, x, y, w, l, yaw, colors[int(cls_pred)])

#         img2d = cv2.imread(img_paths[0])
#         calib = Calibration(img_paths[0].replace(".png", ".txt").replace("image_2", "calib"))
#         objects_pred = predictions_to_kitti_format(img_detections, calib, img2d.shape,  img_size)

#         img2d = show_image_with_boxes(img2d, objects_pred, calib, False)

#         cv2_imshow(RGB_Map)
#         cv2_imshow(img2d)

#         if cv2.waitKey(0) & 0xFF == 27:
#             break

